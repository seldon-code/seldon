[simulation]
model = "ActivityDriven"
# rng_seed = 120 # Leaving this empty will pick a random seed

[io]
n_output_network = 20 # Write the network every 20 iterations
n_output_agents = 1   # Write the opinions of agents after every iteration
print_progress = true # Print the iteration time ; if not set, then does not print

[model]
max_iterations = 500 # If not set, max iterations is infinite

[ActivityDriven]
dt = 0.01               # Timestep for the integration of the coupled ODEs
m = 10                  # Number of agents contacted, when the agent is active
eps = 0.01              # Minimum activity epsilon; a_i belongs to [epsilon,1]
gamma = 2.1             # Exponent of activity power law distribution of activities
reciprocity = 0.65      # probability that when agent i contacts j via weighted reservoir sampling, j also sends feedback to i. So every agent can have more than m incoming connections
homophily = 1.0         # aka beta. if zero, agents pick their interaction partners at random
alpha = 3.0             # Controversialness of the issue, must be greater than 0.
K = 2.0
mean_activities = false # Use the mean value of the powerlaw distribution for the activities of all agents
mean_weights = false    # Use the meanfield approximation of the network edges

reluctances = true       # Assigns a "reluctance" (m_i) to each agent. By default; false and every agent has a reluctance of 1
reluctance_mean = 1.0    # Mean of distribution before drawing from a truncated normal distribution (default set to 1.0)
reluctance_sigma = 0.25  # Width of normal distribution (before truncating)
reluctance_eps = 0.01    # Minimum such that the normal distribution is truncated at this value

[network]
number_of_agents = 1000
connections_per_agent = 10
